# Hadoop

面试题

1.

https://cloud.tencent.com/developer/article/1797979

2.

https://developer.aliyun.com/article/1500308

3.

https://bbs.huaweicloud.com/blogs/301138

4.

https://github.com/MoRan1607/BigDataGuide/blob/master/%E9%9D%A2%E8%AF%95/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89.md

5.

https://www.cnblogs.com/yeyuzhuanjia/p/17074146.html

6.

https://blog.csdn.net/qq_37555071/article/details/120473987

7.

https://www.ikeguang.com/article/1860

8.

https://blog.csdn.net/zuolixiangfisher/article/details/125706515

9.

https://www.cnblogs.com/the-pig-of-zf/p/17300348.html

10.

https://cloud.tencent.com/developer/article/2212535

# Spark

面试题：

https://www.cnblogs.com/crazymakercircle/p/17539112.html

https://www.iamshuaidi.com/26338.html

https://www.iamshuaidi.com/26338.html

https://github.com/wangzhiwubigdata/God-Of-BigData/tree/master/%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97/Spark%E9%9D%A2%E8%AF%95%E9%A2%98%E6%95%B4%E7%90%86

https://gitee.com/yoodb/ebooks/blob/master/docs/Spark/2022%E5%B9%B4%E6%9C%80%E5%85%A8Spark%E9%9D%A2%E8%AF%95%E9%A2%98%E9%99%84%E7%AD%94%E6%A1%88%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%B1%87%E6%80%BB.md

## Spark程序提交运行流程

**Yarn client 模式**：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8c1fa84436ff4d3eb69fe03517650412.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/de0d987f24cd41558b7d3a2536ec4c83.png)

```
1、在 Yarn client 模式下，通过 spark-submit 提交程序后，会在 client 服务器运行 main() 函数，启动 Dirver 进程；
2、Driver 开始构建并初始化 SparkContext ；
3、SparkContext 向 ClusterManager（ResourceManager）注册，并申请运行 Executor 的资源（内核和内存）；
4、ClusterManager 根据 SparkContext 提出的申请和 Worker（NodeManager） 的心跳报告，来决定在哪个 Worker 上启动 Executor；
5、Worker 节点收到请求后会启动 Executor；
6、Executor 向 SparkContext 注册，这样 Driver 就知道哪些 Executor 运行该应用；
7、SparkContext 构建 DAG 图，DAG Scheduler 将 DAG 图分解成多个 Stage ，并把每个 Stage 的 TaskSet 发送给 Task Scheduler ；
8、Executor 向 SparkContext 申请 Task ，Task Secheduler 将 Task发送给 Executor，同时 SparkContext 将程序代码发送给 Executor；
9、Task 在 Executor 上运行，把运行结果反馈给 Task Scheduler，然后再反馈给 DAG Scheduler，运行完毕后写入数据；
10、SparkContext 向 ClusterManager 注销并释放所有资源；
```

**Yarn cluster 模式**：

![img](https://i-blog.csdnimg.cn/direct/7c4b1805976b4f7496efab1a1f7d439f.png)

```
1、在 Yarn cluster 模式下，通过 spark-submit 提交任务后，会启动一个临时进程；
2、临时进程向 ClusterManager（ResourceManager） 通信申请启动 ApplicationMaster；
3、ClusterManager分配 container，并通知 NodeManager 启动 ApplicationMaster，ApplicationMaster启动，Application Master负责资源管理，Application Master向ResourceManager请求集群资源，确保有足够的资源运行 Spark 作业，Application Master为Driver程序分配资源，运行Driver程序；
4、NodeManager 启动 Driver，Driver会与 Application Master 和 Executor节点进行通信，调度任务并收集结果。
5、Driver 启动后开始运行用户 main() 函数；
6、Driver开始构建 SparkContext；
7、SparkContext 向 ClusterManager注册 Application 并申请运行 Executor 的资源；
8、ClusterManager收到 Driver 的资源申请后会分配合适的 Worker 节点；
9、Worker 节点启动 Executor 进程；
10、Executor 进程启动后会向 SparkContext 反向注册；
11、SparkContext 构建 DAG 图，DAG Scheduler 将 DAG 图分解成多个 Stage ，并把每个 Stage 的 TaskSet 发送给 Task Scheduler ；
12、Executor 向 SparkContext 申请 Task ，Task Secheduler 将 Task发送给 Executor，同时 SparkContext 将程序代码发送给 Executor；
13、Task 在 Executor 上运行，把运行结果反馈给 Task Scheduler，然后再反馈给 DAG Scheduler，运行完毕后写入数据；
14、SparkContext 向 ClusterManager 注销并释放所有资源；
```

## 1.spark计算为什么比Mapreduce快？

![image-20241111163424254](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241111163424254.png)

## 2.如何在 Spark Streaming 中消费 Kafka 数据并保证不丢数据

![image-20241112184006856](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241112184006856.png)

## 3.spark在消费kafka数据时kafka集群挂了会丢数据吗

![image-20241112184422932](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241112184422932.png)

![image-20241112184438103](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241112184438103.png)

## 4.kafka是怎么保证数据不丢失的

![image-20241112185410851](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241112185410851.png)

![image-20241112185630403](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241112185630403.png)

![image-20241112185645444](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241112185645444.png)

## 5.数据倾斜

在 Spark 作业中，数据倾斜（Data Skew）是指某些分区的数据量或计算量远超其他分区，导致任务执行时间过长，成为性能瓶颈。数据倾斜通常发生在 `join`、`groupByKey`、`reduceByKey` 等操作中。以下是数据倾斜的定位和解决方法：

### 一、数据倾斜的定位

1. **查看任务执行时间**：
   - 通过 Spark UI 监控各个阶段的执行情况，定位耗时较长的阶段或任务，特别是 `shuffle` 阶段，容易发生数据倾斜。
   - 在 Stage 视图中，如果某些 Task 执行时间远高于平均水平，说明可能存在数据倾斜。
2. **分析数据分布**：
   - 通过对数据的 `key` 值进行采样，观察不同 `key` 的数据量，判断是否有热点 `key`（某些 `key` 数据量特别大），这是造成数据倾斜的常见原因。
   - 可以用 RDD 或 DataFrame 的 `groupBy` 或 `count` 操作，统计各 `key` 的数据量分布情况。
3. **检查分区数据大小**：
   - 使用 `glom()` 将 RDD 转换为 Array，然后对比各个分区的元素数量。如果某些分区的数据量远大于其他分区，说明有可能存在数据倾斜。

### 二、数据倾斜的解决方法

1. **随机前缀法**

对倾斜的 `key` 添加随机前缀，将数据分散到多个分区中，缓解倾斜问题。该方法适用于 `join` 或 `reduceByKey` 操作的倾斜场景。

- 步骤：

  1. 给倾斜的 `key` 添加随机前缀，将数据打散。
  2. 处理完分散的数据后，去除随机前缀还原 `key`。

- 示例：

  ```
  scala
  
  
  Copy code
  val skewedKeys = rdd.filter(x => x._1 == "hot_key")  // 假设 "hot_key" 是倾斜的key
  val normalKeys = rdd.filter(x => x._1 != "hot_key")
  
  val skewedWithPrefix = skewedKeys.map(x => ((Random.nextInt(10), x._1), x._2))
  val result = skewedWithPrefix.reduceByKey(_ + _).map(x => (x._1._2, x._2)).union(normalKeys)
  ```

2. **扩大分区数**

通过增加分区数来降低每个分区的数据量，使每个 Task 的数据量减少。适用于 `groupByKey`、`reduceByKey` 等操作。

- 示例：

  ```
  scala
  
  
  Copy code
  val result = rdd.reduceByKey(_ + _, numPartitions = 100)  // 增加分区数
  ```

3. **使用 `map-side` 聚合**

对于 `reduceByKey` 和 `aggregateByKey` 等聚合操作，先进行 `map-side` 聚合（在 `shuffle` 之前），减少传输的数据量。

- 示例：

  ```
  scala
  
  
  Copy code
  val result = rdd.mapValues(value => (value, 1))
    .reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)) // 在 map 端先聚合一次
    .mapValues { case (sum, count) => sum / count }
  ```

4. **广播小表**

在 `join` 操作中，如果其中一个表较小（如小于 10 MB），可以将小表广播到所有节点，避免 `shuffle` 操作。适用于小表与大表的 `join` 场景。

- 示例：

  ```
  scala
  
  
  Copy code
  import org.apache.spark.sql.functions.broadcast
  
  val largeDf = spark.read.parquet("large_table")
  val smallDf = spark.read.parquet("small_table")
  val result = largeDf.join(broadcast(smallDf), "key")
  ```

5. **使用 `salting` 方法**

对倾斜的 `key` 添加固定的前缀（salt），然后进行连接或聚合，再去掉前缀。适用于 `join` 或 `reduceByKey` 中热点 `key` 导致的倾斜问题。

- 步骤：

  1. 给热点 `key` 人为添加前缀，将其分散到不同的分区。
  2. `join` 后去掉前缀还原 `key`。

- 示例：

  ```
  scala
  
  
  Copy code
  val saltedData = rdd.map { case (key, value) =>
    val salt = new Random().nextInt(10)
    (key + "_" + salt, value)
  }
  val reducedData = saltedData.reduceByKey(_ + _)
  val result = reducedData.map { case (saltedKey, value) =>
    val originalKey = saltedKey.split("_")(0)
    (originalKey, value)
  }
  ```

6. **自定义分区器**

对于倾斜明显的数据，可以采用自定义分区器，使特定 `key` 的数据分配到不同的分区，减轻热点分区的压力。

- 示例：

  ```
  scala
  
  
  Copy code
  import org.apache.spark.Partitioner
  
  class CustomPartitioner(partitions: Int) extends Partitioner {
    def numPartitions: Int = partitions
  
    def getPartition(key: Any): Int = {
      key match {
        case "hot_key" => new Random().nextInt(partitions) // 针对热点key随机分区
        case _ => key.hashCode() % partitions // 其他key按默认hash分区
      }
    }
  }
  
  val partitionedRDD = rdd.partitionBy(new CustomPartitioner(10))
  ```

7. **启用动态分区裁剪**

在 Spark SQL 中，可以启用动态分区裁剪（DPP），减少 `join` 操作时扫描不必要的数据。适用于 `Spark SQL` 中的 `join` 操作。

- 启用方式：

  ```
  spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
  ```

总结

数据倾斜问题的解决需要针对不同的场景选择不同的策略。对于 `join` 操作，可以尝试广播小表或使用 `salting` 方法。对于聚合操作，可以采用 `map-side` 聚合、增加分区数等方式。通过合理的调优措施，可以有效减少数据倾斜对任务执行时间的影响，提升 Spark 作业的整体性能。

## 6.spark性能优化

https://cloud.tencent.com/developer/article/2367444

```
Spark 性能优化是为了提升 Spark 作业的执行效率、减少资源消耗和加快数据处理速度。优化手段涵盖了代码、配置、资源分配等多个方面，以下是一些常用的 Spark 性能优化方法：

1. 数据存储优化
数据分区：根据数据的大小和结构，合理分配分区数，以平衡各分区的数据量。对于较大的数据集，适当增加分区数可以提高并行度，减轻单个分区过大的问题。
使用合适的存储格式：使用列式存储格式（如 Parquet、ORC）能够减少磁盘 I/O，特别是在 Spark SQL 中，有助于提升查询性能。
数据压缩：选择合适的压缩算法（如 Snappy）可以减少磁盘占用和传输时间，但需要在压缩比和解压速度之间取得平衡。
过滤无效数据：通过 filter 等操作提前过滤掉无效数据，避免不必要的计算。
2. 作业优化
缓存与持久化：对频繁使用的数据集使用缓存（cache 或 persist），避免重复计算。缓存可以将数据存储在内存中，减少后续的计算时间。
窄依赖和宽依赖：尽量减少宽依赖（Shuffle），增加窄依赖（如 map、filter）操作。宽依赖需要进行跨节点的 Shuffle 操作，代价较高。
避免 UDF：Spark UDF（用户定义函数）在执行时会增加序列化、反序列化的开销，并且 Spark 无法优化其内部逻辑。可以用内置函数替代 UDF。
广播小表：在 join 操作中，如果一个表很小，可以将其使用 broadcast 函数广播到每个节点，这样可以避免 Shuffle 操作，提高连接效率。
3. 资源配置优化
调整并行度：使用 spark.default.parallelism 和 spark.sql.shuffle.partitions 配置项来设置并行度。并行度越高，任务的粒度越小，能够更好地利用资源，但过高的并行度可能会导致过多的小任务开销。
动态资源分配：通过配置 spark.dynamicAllocation.enabled，Spark 可以根据任务的需要动态申请和释放资源，从而更高效地利用集群资源。
调整内存设置：为执行器（Executor）分配足够的内存，同时合理配置 spark.executor.memory、spark.driver.memory、spark.memory.fraction 等内存参数，确保内存利用率最大化且避免 OOM（内存溢出）错误。
4. Shuffle 优化
减少 Shuffle 操作：尽量使用带有 Combine 功能的算子（如 reduceByKey、aggregateByKey），避免 groupByKey 等会直接触发 Shuffle 的算子。
合并小文件：小文件过多会导致 Shuffle 的开销增大，影响性能。可以调整 spark.sql.files.maxPartitionBytes 和 spark.sql.files.openCostInBytes 来控制小文件的读取方式。
启用 Tungsten 引擎：Tungsten 引擎能提高 Shuffle 的性能，特别是通过减少序列化和反序列化开销、减少 GC 开销。Tungsten 引擎是 Spark 2.x 及以上版本的默认配置。
5. 数据倾斜处理
随机前缀法：在 join 或 reduceByKey 等操作中，对热点数据（某些 key 出现频率极高）进行随机前缀添加，使数据分布更加均匀，减少单个节点的压力。
分区器调整：在 reduceByKey 等操作中自定义分区器，以便对数据进行更加合理的分区，避免数据倾斜。
使用 salting 方法：通过给某些 key 添加随机值（salt），将其分散到不同的分区中，然后在后续的聚合步骤中去掉 salt。
6. JVM 优化
序列化方式：将序列化方式从 Java 默认的 JavaSerializer 改为 KryoSerializer，Kryo 的序列化效率更高。
减少 GC 开销：设置合适的内存分配和 GC 选项，避免 JVM 内存回收带来的延迟。可以通过 spark.executor.memoryOverhead 来为每个 Executor 预留额外的内存。
7. SQL 查询优化
使用 Catalyst 优化器：Catalyst 优化器自动优化查询逻辑（如列剪裁、谓词下推等），用户应尽量使用 Spark SQL 内置的功能来完成复杂的计算。
启用谓词下推：Spark SQL 会将过滤条件下推到数据源端执行，减少需要传输和处理的数据量。可以通过 spark.sql.pushdown 配置来控制。
启用动态分区裁剪（DPP）：在进行大表 Join 操作时，Spark 可以使用动态分区裁剪来减少不必要的数据扫描，显著提高 Join 查询的性能。
减少列扫描：只读取必要的列，避免扫描整个数据集的所有列，尤其在列式存储（如 Parquet）格式下，减少了 I/O 开销。
8. 代码优化
链式操作：将多个相同的数据操作链式组合，避免重复生成 RDD 或 DataFrame，从而减少 DAG 的复杂度。
避免创建过多的临时变量：创建过多的 RDD 或 DataFrame 会导致任务 DAG 过于复杂，因此应尽量复用变量或链式操作。
简化 DAG 图：复杂的 DAG 可能导致计划阶段的时间增加，可以通过将一些中间结果 cache 或 checkpoint 到磁盘来简化 DAG 图，避免 DAG 过长而导致任务重启困难。

总结
Spark 性能优化的关键在于减少 Shuffle 开销、合理利用内存和计算资源、优化数据存储格式、简化作业的执行逻辑。在实际应用中，可以通过分析作业的执行计划、评估内存和 CPU 的利用情况，结合上述优化手段来提高 Spark 作业的整体性能。
```

## 7.Hash Join 与 Sort Merge Join 的区别

![image-20241113194445714](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241113194445714.png)

## 8.spark shuffle类型和区别

在 Spark 中，**Shuffle** 是指将数据从一个任务重新分配到另一个任务的过程，通常涉及跨节点的数据传输。根据 Shuffle 数据的处理方式和场景不同，Spark 有几种不同类型的 Shuffle 操作，每种操作有其特点和适用场景。主要的 Shuffle 类型包括 **Map-side Shuffle** 和 **Reduce-side Shuffle**。

### 一、 **Shuffle 类型**

(1) **Map-side Shuffle**

Map-side Shuffle 是 Spark 中一种高效的 Shuffle 类型，它发生在 Map 阶段，具体来说是当数据被分配到不同的分区时。Map-side Shuffle 会在执行某些操作（如 `reduceByKey`）时预先进行局部聚合，减少 Shuffle 时的数据量。

- **操作类型**：如 `reduceByKey`、`aggregateByKey`、`groupByKey`（某些情况下）。
- **特点**：每个任务会在本地进行一定量的数据预聚合，减少需要通过网络传输的数据量，从而减少 Shuffle 过程中的开销。

在这种类型的 Shuffle 中，如果对相同键的数据进行聚合操作（如求和、计数），这些数据会在 Map 阶段就被部分合并成更小的数据块，这样可以减少网络传输的数据量，从而减少 Shuffle 开销。

(2) **Reduce-side Shuffle**

Reduce-side Shuffle 发生在 **Reduce 阶段**，它通常是指在数据经过 Map 阶段处理后，数据会被发送到其他节点进行聚合操作。数据在 Shuffle 过程中会被完全重新分区并按键进行排序，以保证数据可以正确地聚合。

- **操作类型**：如 `groupByKey`、`join`、`cogroup`。
- **特点**：在 Reduce 阶段，所有相同的键都会被送到同一个 Reduce 任务中进行处理，因此需要进行跨节点的数据传输。这个过程的开销相对较大。

在 Reduce-side Shuffle 中，Spark 会将所有键的值传输到一个节点，以便在 Reduce 阶段进行聚合或操作。该过程可能会涉及数据倾斜问题，尤其是当某些键的值特别多时。

### 二、 **Shuffle 的实现方式**

Spark 通过两种主要的方式来实现 Shuffle：**Sort-based Shuffle** 和 **Hash-based Shuffle**。

(1) **Sort-based Shuffle**

Sort-based Shuffle 是 Spark 默认的 Shuffle 方式，通常用于 Spark SQL 或基于排序的聚合操作，如 `groupBy` 和 `join`。

- **实现原理**：这种方式首先将数据按键排序，然后根据排序结果将数据划分到不同的分区，进行跨节点的数据传输。
- **优点**：这种方式适合需要排序的操作，例如 `sortByKey`，同时也避免了大量的哈希冲突和数据倾斜。
- **缺点**：需要对数据进行排序，因此会消耗额外的计算资源，且排序过程可能导致内存使用过多。

(2) **Hash-based Shuffle**

Hash-based Shuffle 是一种基于哈希的 Shuffle 方式，常用于类似 `reduceByKey`、`aggregateByKey` 等操作。

- **实现原理**：在 Hash-based Shuffle 中，数据按照哈希值进行分区，即每个键都经过哈希函数映射到一个特定的分区。此方式不需要对数据进行排序。
- **优点**：性能更高，尤其是在数据量大时，因为不需要进行排序。
- **缺点**：存在数据倾斜的风险，尤其是当某些键的哈希值映射到同一个分区时，可能导致某些分区的数据量过大，影响性能。

### 三、 **Shuffle 的性能问题与优化**

Shuffle 是 Spark 中性能瓶颈的常见来源，因此对 Shuffle 过程的优化非常重要。常见的优化方法包括：

(1) **减少 Shuffle 的数据量**

- 使用 `reduceByKey` 或 `aggregateByKey` 等操作代替 `groupByKey`，因为后者需要将所有数据传输到一个节点进行聚合。
- 在执行 `join` 操作时，可以考虑使用广播 Join，特别是当一张表非常小而另一张表很大时。

(2) **调整分区数**

- 调整 `spark.sql.shuffle.partitions`（默认值 200），可以根据数据量的大小来调整 Shuffle 的分区数，从而提高性能。

(3) **避免数据倾斜**

- 数据倾斜指的是某些分区的数据量远大于其他分区，这会导致某些节点处理数据过慢。可以通过添加随机前缀（salting）等方式来分散热点数据，减轻倾斜。

(4) **调整内存和磁盘管理**

- 适当调整 `spark.shuffle.file.buffer` 和 `spark.shuffle.memoryFraction` 等配置，确保 Shuffle 数据在内存中有足够的缓存空间，减少磁盘 I/O。

(5) **开启并行度和动态分配**

- 通过调整 `spark.default.parallelism` 和启用动态分配（`spark.dynamicAllocation.enabled`）来优化 Shuffle 的并行度。

### 四、 **总结：Shuffle 类型及区别**

- **Map-side Shuffle**：是高效的，尤其在处理聚合时，减少了网络传输的数据量。
- **Reduce-side Shuffle**：涉及大规模的数据传输和跨节点的聚合，通常更加耗时，容易引发数据倾斜问题。
- **Hash-based Shuffle**：使用哈希分区，不需要排序，适合大量的聚合操作，但容易产生数据倾斜。
- **Sort-based Shuffle**：通过排序进行分区，适用于需要排序的操作，但开销较大。

## 9.小文件

### 一、小文件的产生原因

```
在大数据处理系统（如 Hadoop、Spark 等）中，小文件是指比 HDFS 数据块大小（通常是 64MB 或 128MB）更小的文件。这些小文件在实际处理中会带来性能问题，主要原因包括：

1. 数据分片不合理
数据被切分成了大量的小分区或小文件，未达到 HDFS 的块大小。
如 Spark 中过多的分区导致每个分区生成一个小文件。
2. 数据倾斜
数据分布不均匀，有些分区数据量很小，而另一些分区数据量很大，导致部分分区生成小文件。
3. 不恰当的分区配置
默认分区数（numPartitions）过多，或手动设置的分区数不合理，导致每个分区生成的小文件量过多。
4. 数据导入问题
从外部系统导入数据（如 Kafka、Flume 等）时，没有进行合并处理，直接将小文件写入 HDFS。
5. 任务执行失败或异常中断
Spark 或 Hadoop 的任务中断时，会导致部分任务生成的临时文件没有被合并，形成小文件。
6. 压缩方式的影响
对数据进行了压缩，但文件总量较小，导致最终生成多个小文件。
7. 流式任务实时写入
流式处理系统（如 Spark Streaming、Flink）按时间窗口写入数据，每个窗口可能只写入少量数据，最终生成大量小文件。
```

### 二、小文件的影响

```
1.占用大量 NameNode 内存
HDFS 的文件和块信息存储在 NameNode 的内存中，小文件过多会占用过多的内存资源，影响集群性能。
降低读写性能

2.小文件会导致大量随机 I/O 操作，降低读写效率。
在 MapReduce 或 Spark 中读取小文件时，每个文件会启动一个任务，增加任务调度的开销。

3.增加网络传输开销
小文件导致 Shuffle 阶段的数据传输量增加，影响任务的执行效率。

4加重磁盘压力
小文件多会导致磁盘频繁操作，增加硬盘 I/O 压力。
```

### 三、小文件的解决方案

```
1.数据写入时的优化
1.1合理设置分区数
在 Spark 中，可以通过调整分区数来减少小文件的数量：
rdd.repartition(适当的分区数) 
或者
rdd.coalesce(较少的分区数, shuffle = true)

1.2批量写入数据
流式任务中，可以通过合并小批量数据后再写入，避免生成小文件。
设置时间窗口或缓冲区大小：
如 Flink 的 bulk.add。
Spark Streaming 使用较大的 batchInterval。

1.3设置文件输出格式
使用支持压缩和合并的输出格式，例如 Parquet、ORC。

1.4动态分区调整
Spark SQL 提供动态分区调整：
SET spark.sql.adaptive.enabled = true;

2. 数据合并处理
3. 数据倾斜的优化
4. 合理的压缩策略
5. 流式任务的优化
```

### 四、总结

小文件问题主要由数据分布不均、分区不合理或写入机制不当引起。通过调整分区数、合并文件、优化数据存储格式等手段，可以有效减少小文件对性能的影响。

> **核心原则：**
>
> - 合理分区，减少任务开销。
> - 合并小文件，提高存储效率。
> - 压缩数据，减少磁盘和网络传输压力。



## 10.Spark 中 `distinct()` 实现原理

**(1)RDD.distinct() 的底层实现**

Spark 的 `distinct()` 操作实际上是一个 **聚合操作**，它的核心是通过 **MapReduce** 模型来去重。

`distinct()` 是通过调用 `map` 和 `reduceByKey` 实现的，底层逻辑如下：

```
*map：对每个元素生成 (key, null) 的键值对，其中 key 是元素本身。
*reduceByKey：按 key 聚合，保留唯一的键（即去重）。
*map：丢弃 null 值，仅保留 key。
```

代码示例（简化实现）：

```scala
rdd.map(x => (x, null))      // 转换为键值对 (key, null)
   .reduceByKey((_, _) => null) // 按 key 聚合，去重
   .map(_._1)               // 提取去重后的 key
```

**核心步骤：**

1. **局部去重**：在每个分区内对数据进行哈希映射，保留分区内唯一值，减少数据量。
2. **Shuffle**：将所有分区内的数据通过哈希分区器重新分配到对应的分区（同一个键到同一分区）。
3. **全局去重**：在目标分区对键进行合并，最终只保留唯一的键。



**(2)DataFrame.distinct() 的底层实现**

**DataFrame API** 的 `distinct()` 操作与 RDD 类似，但会通过 Catalyst 优化器生成更高效的执行计划。

**逻辑计划**

`distinct()` 在 Catalyst 的逻辑计划中被表示为 `Aggregate` 操作，等价于对所有列执行 `GROUP BY`，保留唯一的行。

**物理计划**

物理计划中，`distinct()` 会被优化为基于 `HashAggregate` 或 `SortAggregate` 的操作：

1. **HashAggregate**：基于内存哈希表分组，适合内存充足、数据基数较低的场景。
2. **SortAggregate**：对分组键排序后去重，适合数据量大或内存不足的场景。

**步骤：**

1. **局部去重**（Partial Aggregation）：在每个分区中计算唯一值，尽量减少 `Shuffle` 数据量。
2. **全局去重**（Final Aggregation）：通过 `Shuffle` 操作将所有分区的数据按分组键重新分配，再进行最终的去重。



**`distinct()` 的 Shuffle 开销**

在分布式环境中，去重操作需要全局对数据进行比较，这不可避免地涉及 **Shuffle**。以下是 `distinct()` 的 Shuffle 逻辑：

1. 局部分区内去重：
   - 每个分区会通过哈希表记录本地唯一值。
2. 跨分区去重（Shuffle）：
   - 数据通过哈希分区器按照键重新分配到不同分区。
   - 同一个值（或行）会被发送到同一个分区。
3. 分区内最终去重：
   - 在目标分区中，对接收到的键（或行）进行去重，保留唯一值。



**优化策略**

由于 `distinct()` 涉及全局 `Shuffle`，当数据量较大或分区不均衡时，性能可能会受到显著影响。以下是常见的优化策略：

**（1）提前过滤数据**

尽可能提前过滤掉不需要的数据，减少参与去重的记录数：

```scala
rdd.filter(condition).distinct()
```

**（2）增加 Shuffle 分区数**

通过增加 `Shuffle` 分区的数量，可以均衡分区负载，避免数据倾斜：

```scala
rdd.distinct(numPartitions = 200)
```

对于 DataFrame，可以设置：

```scala
spark.conf.set("spark.sql.shuffle.partitions", 200)
```

**（3）使用局部聚合**

在某些场景下，可以通过自定义逻辑先在每个分区内完成局部去重，再进行全局去重。

**（4）考虑 Approximate Distinct**

如果对去重的精确性要求不高，可以使用近似去重算法（如 HyperLogLog），避免全局 Shuffle。例如：

# 数据仓库

面试题：

https://blog.csdn.net/qq_30031221/article/details/116500095

https://xie.infoq.cn/article/e63c84e6747df50fd319a8c67

https://developer.aliyun.com/article/931940

## 1.数据仓库和数据中台区别

![image-20241113094412347](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241113094412347.png)



数据仓库/数据集市/数据中台/数据集成/数据湖



## 2.数仓分层

ODS 原始数据层

DW 数据仓库层（维度建模）

ADS 数据计算层

DIM 维度层

**分层作用**

- 简化复杂问题，易于数据处理流程的理解
- 结构更清晰，使用表时更清晰地理解表
- 数据安全，权限管理，隔离敏感数据
- 数据隔离，屏蔽原始数据异常，计算数据与原始数据解耦合
- 数据血缘，便于问题追踪和调试
- 用空间换时间
- 数据重复使用，減少重复开发
- 增强扩展性和有利于后期维护，不分层可能会导致整个数据处理流程受到源系统业务系统规则变化的影响，使用分层可以降低影响范围

## 3.数仓建模

### 3.1维度建模

维度模型将企业的业务通过两个方面进行数据建模，即事实表和维度表。

**业务过程**:**企业活动中的事件，如下单、支付、退款都是业务过程，业务过程是一个不可拆分的行为事件。**企业是从收益、成本出发，关注价值链条上最具影响力的事情或者事件。



事实表：

在现实世界中，每一个操作型事件，基本都是发生在实体之间的，伴随着这种操作事件的发生，会产生可度量的值，而这个过程就产生了一个事实表，存储了每一个可度量的事件。

事务型事实表

快照型事实表

累计快照型事务表



维度表：

维度，顾名思义，业务过程的发生或分析角度。比如从颜色、尺寸的角度来比较手机的外观，从cpu、内存等较比比较手机性能维。维度表一般为单一主键，在ER模型中，实体为客观存在的事物，会带有自己的 描述性属性，属性一般为文本性、描述性的，这些描述被称为维度。



维度分为：

退化维度

缓慢变化维



**维度建模模型的分类**

星型：每张事实表对应多张维度表，不同事实表相互独立，模型简单，便于分析，但会存在数据冗余

雪花型：多个维度表关联使用，数据冗余较少

星座模型：不同事实表共用同一维度表，企业中最常用

### 3.2范式建模

第一范式：强调原子性，列不可拆分成多列

第二范式：在第一范式的基础上，首先表必须有主键，非主键字段需要完全依赖于主键字段，非主键字段不能只依赖主键的一部分；

第三范式：在第二范式的基础上，非主键列必须**直接依赖**于主键，不能存在传递依赖。即非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。



## 4.数仓建模过程

1. 需求分析，明确业务需求

   **要回答的典型问题**：

   - 哪些指标是需要分析的？（如销售额、利润率）
   - 数据的分析维度有哪些？（如时间、地点、产品等）
   - 分析的时间跨度？

2. 确定业务过程，明确需要建模的业务事件

3. 确定粒度，定义数据的详细程度

4. 确定维度，定义分析的维度表，每个维度表通常包含一个主键和大量的描述性属性。

5. 确定事实表，定义事实表的度量指标

6. 设计维度模型(星型模型)

7. 数据加载和测试，开发 ETL（Extract, Transform, Load）流程，将数据从源系统加载到数据仓库。验证模型和数据是否满足业务需求

8. 优化与维护

## 5.如何保证数仓的安全稳定

```
严格的数据访问控制；
完善的数据备份和恢复机制；
安全的数据存储和传输；
有效的安全审计机制；
人员培训和管理等；
```

# Flink

面试题

https://www.cnblogs.com/crazymakercircle/p/17619152.html



特点：

分布式

可扩展性

Event-time processing事件事件处理:适合乱序场景，做窗口计算,实时计算

Exactly-once State Consistency：有状态计算，且能保证数据只会被处理一次，故障恢复

Checkpoint:高容错,故障恢复



部署模式

Local

Standalone

yarn

k8s



架构：

![image-20250111165818573](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20250111165818573.png)

![img](https://i-blog.csdnimg.cn/blog_migrate/4189486b47433cd8f735edbeaf843d51.png)

一个flink**集群**由一个flink Master和多个flink TaskManager组成,

Flink Master负责处理作业提交，作业监督及资源管理。flink TaskManager是工作（worker）进程，负责执行组成flink作业的实际任务。Flink Master是flink集群的主进程，包含3个组件：

1.Dispatcher：在 Flink 集群中，负责接收作业，负责为新提交的作业启动一个JobManager,Dispatcher 在 Flink 的高可用性部署中起到协调作业启动、停止和恢复的作用。它将作业提交给 JobManager，之后 JobManager 会接管作业的调度和资源分配。

2.ResourceManager：负责资源的管理，在整个flink集群中只有一个ResourceManager，协调 JobManager 和 TaskManager 之间的资源分配。ResourceManager 会向 JobManager 分配 TaskManager 节点，并在任务执行过程中调度资源。

3.JobManager: 

管理作业的运行，在一个flink集群中可能会有多个job同时运行，每个job都有自己的JobManager，JobManager接收作业，调度任务，协调资源分配，生成 JobGraph，将 JobGraph 转为 ExecutionGraph，协调 TaskManager分配 Subtask，以及对失败任务进行恢复和重调度；

**作用**：在 Flink 中，JobManager 是作业调度和协调的核心组件。它负责管理整个作业的生命周期，包括作业的提交、调度、资源管理和状态恢复等。JobManager 负责接收作业的提交，协调任务的执行，管理作业的失败恢复。

**作业调度**：JobManager 将作业划分为多个子任务，并根据资源需求和任务依赖关系调度任务。JobManager 会根据集群的资源情况分配 TaskManager，并将任务调度到 TaskManager 执行。



TaskManager

**作用**：TaskManager 是 Flink 中负责执行任务的工作节点。每个 TaskManager 启动多个任务槽（Task Slot），并通过这些槽来执行不同的任务。TaskManager 负责从 JobManager 接收任务，并实际执行这些任务。

**任务调度**：TaskManager 接收到任务后，负责任务的执行，并向 JobManager 反馈任务的执行状态。

每个worker（TaskManager）都是一个JVM进程，可以在单独的线程里执行一个或者多个子任务(task)，每个TaskManager至少有一个task slot. worker里的子任务共享一个JVM，通过slot隔离内存，不隔离cpu,实际任务中，可以配置任务并行数=slot数，可以优化资源利用率，如下图slot总个数为6（slot个数<=总cpu核数）：

1）并行度=2时

![img](https://i-blog.csdnimg.cn/blog_migrate/ec69e22ce46b1f3f5da2a45fb397e29e.png)

2）并行度=6时

![img](https://i-blog.csdnimg.cn/blog_migrate/4b785f1df1715e9872b1d95e9eb55d3f.png)

![img](https://i-blog.csdnimg.cn/blog_migrate/e4a5b3dbf1a25c83b8cc0b42862a02c6.png)

# Hive SQL

面试题

1.

https://github.com/BigDataScholar/TheKingOfBigData/blob/master/note/%E9%9D%A2%E8%AF%95%E6%9D%80%E6%8B%9B/%E7%9C%8B%E5%AE%8C%E4%BA%86108%E4%BB%BD%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E6%88%91%E4%B8%BA%E4%BD%A0%E6%80%BB%E7%BB%93%E5%87%BA%E4%BA%86%E8%BF%99%2010%20%E4%B8%AA%E3%80%90Hive%E3%80%91%E9%AB%98%E9%A2%91%E8%80%83%E7%82%B9%EF%BC%88%E5%BB%BA%E8%AE%AE%E6%94%B6%E8%97%8F%EF%BC%89.md

2.

https://www.cnblogs.com/crazymakercircle/p/17573653.html

3.

https://blog.csdn.net/qq_37555071/article/details/120475123

4.

https://cloud.tencent.com/developer/article/1972922

5.

https://blog.csdn.net/weixin_43161811/article/details/123947852

6.

https://xie.infoq.cn/article/2c1dfcd4e3291116c1f9e80bf

## 1.sql优化

主要优化点：

1.冗余优化

- 多分区冗余计算
- 历史静态数据重复计算
- 多作业相同逻辑重复执行
- 尽量避免使用select *
-  相同计算逻辑冗余扫描

2.逻辑优化

- 筛选后置导致无用计算
- group by代替distinct
- 单个SQL计算复杂度过高
- 优化重复计算
- cache

3.清除无效作业





## 2.资源优化



# Java

面试题：

https://javaguide.cn/java/basis/java-basic-questions-01.html

https://javabetter.cn/sidebar/sanfene/javase.html

https://cloud.tencent.com/developer/article/2183300

https://www.cnblogs.com/lhboke/p/18366195

https://www.xiaolincoding.com/interview/java.html



# 设计模式

面试题：

https://javabetter.cn/sidebar/sanfene/shejimoshi.html

https://juejin.cn/post/6844904125721772039

https://www.cxyxiaowu.com/16297.html

https://www.cnblogs.com/stry/p/17028470.html

## 分类总结

| **模式类别** | **模式名称** | **示例场景**                |
| ------------ | ------------ | --------------------------- |
| **创建型**   | 单例模式     | **数据库连接池**            |
|              | 工厂方法     | **日志系统**                |
|              | 抽象工厂     | **GUI 组件**                |
|              | 建造者模式   | **复杂对象（汽车）**        |
|              | 原型模式     | **对象克隆**                |
| **结构型**   | 适配器模式   | **电源插头转换器**          |
|              | 桥接模式     | **颜色 + 形状组合**         |
|              | 装饰器模式   | **日志、权限管理**          |
|              | 外观模式     | **简化 API**                |
|              | 享元模式     | **字符缓存、棋子对象**      |
|              | 代理模式     | **远程代理、权限控制**      |
| **行为型**   | 策略模式     | **支付方式（支付宝/微信）** |
|              | 观察者模式   | **事件监听（GUI、订阅）**   |
|              | 责任链模式   | **权限校验**                |
|              | 命令模式     | **按钮操作（撤销/重做）**   |
|              | 中介者模式   | **聊天室、组件交互**        |
|              | 迭代器模式   | **遍历集合**                |
|              | 模板方法     | **数据导入、AI 逻辑**       |
|              | 备忘录模式   | **撤销功能**                |
|              | 访问者模式   | **编译器、报表生成**        |
|              | 解释器模式   | **SQL 解析、规则引擎**      |

## 实际应用

| **应用场景**                | **使用的设计模式**      |
| --------------------------- | ----------------------- |
| **数据库连接池**            | **单例模式**            |
| **日志系统**                | **工厂方法 + 单例**     |
| **Spring IoC**              | **工厂模式 + 依赖注入** |
| **支付方式（支付宝/微信）** | **策略模式**            |
| **消息推送（事件订阅）**    | **观察者模式**          |
| **游戏开发（UI 组件交互）** | **中介者模式**          |
| **GUI 按钮撤销/重做**       | **命令模式**            |
| **文件格式转换**            | **访问者模式**          |
| **SQL 解析器**              | **解释器模式**          |



## 一、单例模式

### 1. `__new__()` (***)

```python
class Singleton:
    _instance = None  # 用于存储唯一实例

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:  # 如果实例不存在，则创建
            cls._instance = super().__new__(cls)
        return cls._instance  # 返回唯一实例

# 测试
s1 = Singleton()
s2 = Singleton()

print(s1 is s2)  # True，证明 s1 和 s2 是同一个实例
```

### 2.装饰器（Decorator）

```python
from functools import wraps

def singleton(cls):
    instances = {}

    @wraps(cls)
    def get_instance(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)  # 创建实例
        return instances[cls]

    return get_instance

@singleton
class Singleton:
    pass

# 测试
s1 = Singleton()
s2 = Singleton()

print(s1 is s2)  # True
```

### 3.import机制

**Python 模块（Module）** 只会被加载一次，天然满足单例模式。

**只需创建一个 `singleton.py` 模块**，然后在需要的地方 `import` 即可。

singleton.py

```python
class Singleton:
    def __init__(self):
        self.value = 42  # 示例属性
```

main.py

```python
from singleton import Singleton

s1 = Singleton()
s2 = Singleton()

print(s1 is s2)  # True
```

### 4.线程安全

```python
import threading

class Singleton:
    _instance = None
    _lock = threading.Lock()  # 线程锁，保证只有一个线程创建实例

    def __new__(cls, *args, **kwargs):
        with cls._lock:  # 线程安全
            if cls._instance is None:
                cls._instance = super().__new__(cls)
        return cls._instance

# 测试
def test_singleton():
    s = Singleton()
    print(f"Instance ID: {id(s)}")

# 创建多个线程
threads = [threading.Thread(target=test_singleton) for _ in range(5)]

# 启动线程
for t in threads:
    t.start()

# 等待所有线程完成
for t in threads:
    t.join()
```

## 二、工厂方法模式

通过**工厂类**创建不同类型的数据源，避免直接 `new` 对象，便于扩展。
**📌 适用场景：**

- **数据源适配**（MySQL、Hive、ClickHouse、ElasticSearch）
- **ETL 任务动态调度**（Spark、Flink、Airflow）

✅ **示例：数据库连接工厂**

```python
class DataSourceFactory:
    @staticmethod
    def get_connection(db_type):
        if db_type == "mysql":
            return "MySQL 连接对象"
        elif db_type == "hive":
            return "Hive 连接对象"
        elif db_type == "clickhouse":
            return "ClickHouse 连接对象"
        else:
            raise ValueError("未知数据库类型")

# 获取不同数据库连接
mysql_conn = DataSourceFactory.get_connection("mysql")
hive_conn = DataSourceFactory.get_connection("hive")
```



# Python

面试题：

https://www.cnblogs.com/hechengQAQ/p/17315387.html

https://github.com/jackfrued/Python-Interview-Bible/blob/master/Python%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-%E5%9F%BA%E7%A1%80%E7%AF%87-2020.md

https://juejin.cn/post/6996181253498077215

https://blog.csdn.net/2401_84688180/article/details/138386791

https://cloud.tencent.com/developer/article/2318528

https://developer.aliyun.com/article/656961

https://www.nowcoder.com/discuss/353147141696724992

https://www.yiibai.com/interview/250

https://www.finclip.com/news/f/11844.html

## 1.数据类型转换函数

![image-20241211111556671](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20241211111556671.png)

## 2.装饰器

```python
def wrapper(func)
	def innner(*args,**kwargs)
  	print("111")
    func(*args,**kwargs)
    print("222")
	return innner

@wrapper
def func_a()
	print("aaa")
@func_pro
def func_b()
	print("bbb")
func_a()
func_b()
```

## GIL

- GIL 是 **Global Interpreter Lock** 的缩写，全局解释器锁。指的是 Python 解释器中用来保护**解释器状态**的一种互斥锁。

- GIL 的存在意味着同一时间内，**只有一个线程可以执行 Python 字节码**，即便运行在多核 CPU 上，Python 的多线程也不能真正实现并行。

  **为什么 Python 需要 GIL？**

  Python 中的 **CPython 解释器** 使用了 GIL，主要是为了处理以下问题：

  1. **线程安全**：
     - CPython 中的内存管理（例如垃圾回收器）不是线程安全的。
     - 如果没有 GIL，不同线程可能会同时操作引用计数，导致内存错误。
  2. **简化实现**：
     - 引入 GIL 后，不需要为每个对象的内存操作加锁，从而减少了复杂性。
     - 开发者可以将精力放在实现语言功能上，而不是担心多线程引发的内存问题。

## 异步调用

asyncio

https://blog.csdn.net/m0_73716246/article/details/132214296



# 数据结构与算法

## **数据结构**

线性数据结构

```markdown
数组（Array）
链表（Linked List）
栈（Stack）
队列（Queue）
双端队列（Deque）
```

非线性数据结构

```markdown
哈希表（Hash Table / Dictionary）
树（Tree）
图（Graph）
```



## leetcode

**面试经典 150 题**

https://leetcode.cn/studyplan/top-interview-150/

**Hot 100**

https://leetcode.cn/studyplan/top-100-liked/



## 1.二分查找

```python
# 迭代法
def binary_search_recursive(arr, left, right, target):
    if left > right:
        return -1  # 递归终止条件

    mid = (left + right) // 2
    if arr[mid] == target:
        return mid
    elif arr[mid] < target:
        return binary_search_recursive(arr, mid + 1, right, target)
    else:
        return binary_search_recursive(arr, left, mid - 1, target)

# 测试
arr = [1, 2, 3, 4, 5, 6, 7, 8]
print(binary_search_recursive(arr, 0, len(arr) - 1, 6))  # 输出: 5



#递归法
def binary_search(arr, l, r, t):
    mid = (l + r) // 2
    if (len(arr) == 1 and arr[0] != t) or (l > r):
        return -1

    if arr[mid] == t:
        return mid
    if arr[mid] > t:
        return binary_search(arr, l, mid - 1, t)
    else:
        return binary_search(arr, mid + 1, r, t)


if __name__ == '__main__':
    test_arr = [1,2,3,4,5,6,7,8]
    p = 6
    i = binary_search(test_arr, 0, len(test_arr) - 1, p)
    if i == -1:
        print("无法在数组中找到该元素！")
    else:
        print(f"{p}在数组中的索引为{i}")
```

时间复杂度：O(log n)

空间复杂度：

迭代版： O(1)；

递归版： O(log n)；

## 2.冒泡排序

```python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        swapped = False
        for j in range(n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
                swapped = True
        if not swapped:
            break

    return arr


if __name__ == '__main__':
    test_arr = [5, 4, 3, 2, 6, 7]
    sorted_arr = bubble_sort(test_arr)
    print(sorted_arr)
```

时间复杂度：O(n2)

空间复杂度：O(1)

## 3.快速排序

```py
def quick_sort(arr):
    if arr == [] or len(arr) <= 1:
        return arr
    # 优化
    f = random.choice(arr)
    left_arr = []
    right_arr = []
    for i in arr:
        if i < f:
            left_arr.append(i)
        if i > f:
            right_arr.append(i)
    return quick_sort(left_arr) + [f] + quick_sort(right_arr)


if __name__ == '__main__':
    test_arr = [5, 4, 3, 2, 6, 7]
    sorted_arr = quick_sort(test_arr)
    print(sorted_arr)
```

时间复杂度：O(NlogN)

空间复杂度：O(N)

## 4.归并排序

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    else:
        mid = 0 + len(arr) // 2
        left_arr = arr[:mid]
        left_sorted = merge_sort(left_arr)
        right_arr = arr[mid:]
        right_sorted = merge_sort(right_arr)
        return merge(left_sorted, right_sorted)


def merge(l_arr, r_arr):
    i = j = 0
    sorted_arr = []
    while i < len(l_arr) and j < len(r_arr):
        if l_arr[i] < r_arr[j]:
            sorted_arr.append(l_arr[i])
            i += 1
        else:
            sorted_arr.append(r_arr[j])
            j += 1
    sorted_arr.extend(l_arr[i:])
    sorted_arr.extend(r_arr[j:])

    return sorted_arr


if __name__ == '__main__':
    test_arr = [5, 4, 3, 2, 6, 7]
    print(merge_sort(test_arr))
```

时间复杂度：O(NlogN)

空间复杂度：O(N)

![image-20250208175726863](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20250208175726863.png)

## 5.链表反转 (Reverse Linked List)

迭代法

```python
class ListNode:
    def __init__(self, val=0, next=None):
        self.val = val
        self.next = next

def reverse_list(head):
    prev = None  # 初始时前驱节点为空
    curr = head  # 当前节点为头节点
    
    while curr:
        next_node = curr.next  # 先存下 curr 的下一个节点
        curr.next = prev  # 反转指向
        prev = curr  # prev 前进
        curr = next_node  # curr 前进
    
    return prev  # prev 最终指向新的头结点

# 测试
def print_list(head):
    while head:
        print(head.val, end=" -> ")
        head = head.next
    print("None")

# 构建链表 1 -> 2 -> 3 -> 4 -> 5 -> None
head = ListNode(1, ListNode(2, ListNode(3, ListNode(4, ListNode(5, None)))))

print("原链表：")
print_list(head)

reversed_head = reverse_list(head)

print("反转后的链表：")
print_list(reversed_head)
```

递归法

```python
def reverse_list_recursive(head):
    if not head or not head.next:  # 递归终止条件
        return head
    
    new_head = reverse_list_recursive(head.next)  # 递归反转剩余部分
    
    head.next.next = head  # 让后一个节点指向当前节点
    head.next = None  # 断开当前节点的 next 避免循环引用

    return new_head  # 返回新的头节点

# 测试
head = ListNode(1, ListNode(2, ListNode(3, ListNode(4, ListNode(5, None)))))
print("原链表：")
print_list(head)

reversed_head = reverse_list_recursive(head)
print("反转后的链表：")
print_list(reversed_head)
```

![image-20250208182354506](/Users/olivia.tang/Library/Application Support/typora-user-images/image-20250208182354506.png)

## 6.两数之和

```python
def group_anagrams(arr):
    word_map = {}
    for word in arr:
        sorted_word = str(sorted(word))
        if sorted_word in word_map.keys():
            word_list = list(word_map[sorted_word])
            word_list.append(word)
            word_map[sorted_word] = word_list
        else:
            word_map[sorted_word] = [word]

    return list(word_map.values())


if __name__ == '__main__':
    strs = ["eat", "tea", "tan", "ate", "nat", "bat"]
    print(group_anagrams(strs))
```

## 7.异位词

```python
def group_anagrams(arr):
    word_map = {}
    for word in arr:
        sorted_word = str(sorted(word))
        if sorted_word in word_map.keys():
            word_list = list(word_map[sorted_word])
            word_list.append(word)
            word_map[sorted_word] = word_list
        else:
            word_map[sorted_word] = [word]

    return list(word_map.values())


if __name__ == '__main__':
    strs = ["eat", "tea", "tan", "ate", "nat", "bat"]
    print(group_anagrams(strs))
```



# Shell

面试题：

https://github.com/BigDataScholar/TheKingOfBigData/blob/master/note/shell/Shell%E8%BF%9B%E9%98%B6%E5%BF%85%E4%BC%9A%E7%9A%84%E5%87%A0%E4%B8%AA%E5%B7%A5%E5%85%B7%EF%BC%8C%E4%BD%A0%E9%83%BD%E6%8E%8C%E6%8F%A1%E4%BA%86%E5%90%97(%E9%99%84%E7%9C%9F%E5%AE%9E%E4%BC%81%E4%B8%9A%E9%9D%A2%E8%AF%95%E9%A2%98).md

https://cloud.tencent.com/developer/article/1450329

https://blog.csdn.net/daocaokafei/article/details/125711982

https://www.cnblogs.com/qianjinyan/p/11212214.html

https://linux.cn/article-5607-1.html



# MySQL

https://javaguide.cn/database/mysql/mysql-questions-01.html

https://javabetter.cn/sidebar/sanfene/mysql.html

https://github.com/caokegege/Interview/blob/master/db/%E6%9C%80%E5%85%A8MySQL%E9%9D%A2%E8%AF%9560%E9%A2%98%E5%92%8C%E7%AD%94%E6%A1%88.md

https://www.cnblogs.com/souyunku/p/15633392.html

https://www.xiaolincoding.com/interview/mysql.html

https://xiaolincoding.com/mysql/index/index_interview.html

https://github.com/lifei6671/interview-go/blob/master/mysql/mysql-interview.md

Mysql索引：

https://javaguide.cn/database/mysql/mysql-index.html



# Hbase

面试题：

https://github.com/wangzhiwubigdata/God-Of-BigData/blob/master/%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97/HBase%E9%9D%A2%E8%AF%95%E9%A2%98%E6%95%B4%E7%90%86/HBase.md

https://www.cnblogs.com/crazymakercircle/p/17638626.html

https://blog.51cto.com/u_15288309/3053211

https://mikechen.cc/32593.html

# Kafka

面试题：

https://javabetter.cn/interview/kafka-40.html

https://github.com/IcyBiscuit/Java-Guide/blob/master/docs/system-design/distributed-system/message-queue/Kafka%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93.md

https://github.com/wangzhiwubigdata/God-Of-BigData/blob/master/%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97/Kafka%E9%9D%A2%E8%AF%95%E9%A2%98%E6%95%B4%E7%90%86/Kafka%EF%BC%88%E4%B8%80%EF%BC%89.md

# ZooKeeper

面试题：

https://houbb.github.io/2022/05/10/interview-08-zookeeper

https://cloud.tencent.com/developer/article/1513902

https://github.com/wangzhiwubigdata/God-Of-BigData/blob/master/%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97/Zookeeper%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/Zookeeper.md

https://juejin.cn/post/6844903984814096398

# Redis

面试题：

https://www.xiaolincoding.com/redis/base/redis_interview.html

https://javaguide.cn/database/redis/redis-questions-01.html

https://github.com/CoderLeixiaoshuai/java-eight-part/blob/master/docs/redis/%E7%9C%8B%E5%AE%8C%E8%BF%9920%E9%81%93Redis%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E9%98%BF%E9%87%8C%E9%9D%A2%E8%AF%95%E5%8F%AF%E4%BB%A5%E7%BA%A6%E8%B5%B7%E6%9D%A5%E4%BA%86.md

https://javabetter.cn/sidebar/sanfene/redis.html



# Doris

1.基本概念

MPP数据库，OLAP,安装需编译

2.组件

mysql server 访问

FE: 元数据（web页面默认端口8030）

Follower,Leader：数据读写

Observer：只负责数据读取

BE：实际存储数据（web页面默认端口8040）

Broker：访问外部数据源的进程

3.Docker安装

挂载源码目录，maven仓库，到宿主机本地目录

4.数据库操作

Engine：OLAP(default),Hive...

分区 partition：范围分区(通常为时间列），list分区

分桶 distributed(hash)

多分区



数据模型

1.Aggregate(聚合)模型

key,value(replace,sum,max,min)

数据的聚合发生在三种情况下：

```
1.每批数据的导入阶段，在导入的数据内聚合
2.底层BE compaction
3.数据查询阶段，强制聚合
```

2.Uniq模型(数据会做去重)

3.Duplicated(明细)模型(数据可重复，实际上是聚合模型，replace)



模型的选择场景

聚合模型不适合count(*)场景

数据模型在建表时指定，不可修改



动态分区

历史分区数据保留策略和数据热存储；

参数配置：建表时指定或通过alter

查看分区: SHOW partitions from table_name;



Rollup(提高查询速度，物化索引)

适用于聚合模型，基于base表获取更粗粒度的聚合数据，加快查询，不能基于明细模型做预聚合

Duplicate模型：调整列顺序，以命中

语法：alter table add rollup



前缀索引

数据会根据建表时指定的key列进行拼接进行排序存储

使用rollup调整字段顺序调整前缀索引



物化视图

保存查询结果，预存储

比rollup更灵活,功能更强大

经常使用的查询保存下来

语法：create materialized view

暂时只支持单表，不支持join

最优路径选择

查询改写



**删除数据(分区)**

delete from where

Alter table drop partition（推荐）



数据导入

broker load(如hdfs)

基本语法：load label label_name

with broker 

...



Stream load（http协议，小文件）

基本语法：curl --location-trusted -u user:passwd -H -T -X PUT



Routine load(常驻线程，不间断导入数据，如导入kafka数据)

基本语法：create routine load [db.]job_name on tbl_name

[merge_type]

[load_properties]

[job_properties]

Insert(不推荐在生产使用)

Multi load(http协议)



S3 load(S3)





查询

join

explain graph

1.broadcast join(等值关联默认使用,表不能太大)

2.shuffle join

3.Colocation join(提供本地优化，减少数据在节点传输耗时，加快查询，数据关联本地化)

有使用限制：

建表时两张表的分桶列的类型和数量要完全一致，并且桶数一致

所有分区的副本数一致

4.bucket-shuffle-join

A join B on a.分桶列=b. 列

数据传输只需要对b.列取hash发送到A表的不同桶里，相比1，2网络开销和内存开销最小，相对于3，没有强制性要求，更灵活

只能保证左表为单分区时生效



Runtime filter

作用：动态生成过滤条件下推到存储引擎，减少扫描的数据量

runtime_filter_type:

in

bloomfilter

Min-max

参数设置

注意事项：

1.只支持等值连接

2.不支持下推到left outer,full outer, anti join的左表

3.不支持常量表达

....



函数

https://doris.apache.org/zh-CN/docs/sql-manual/sql-functions/date-time-functions/now



# WordCount

## 1.Spark

```scala
import org.apache.spark.sql.SparkSession

object WordCount {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder.appName("WordCount").master("local[*]").getOrCreate()
    val sc = spark.sparkContext

    val input = sc.textFile("hdfs://path/to/input")
    val wordCounts = input.flatMap(_.split("\\s+"))
                          .map(word => (word, 1))
                          .reduceByKey(_ + _)

    wordCounts.saveAsTextFile("hdfs://path/to/output")

    spark.stop()
  }
}
```

## 2.Flink

**2.1使用 Flink DataStream API（流处理）**

```scala
import org.apache.flink.api.common.functions.FlatMapFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

object WordCountStream {
  def main(args: Array[String]): Unit = {
    // 1. 创建 Flink 执行环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    // 2. 从 Socket 读取数据（模拟流式输入）
    val textStream: DataStream[String] = env.socketTextStream("localhost", 9999)

    // 3. 处理数据
    val wordCounts = textStream
      .flatMap(_.toLowerCase.split("\\s+"))
      .map((_, 1))
      .keyBy(_._1)
      .sum(1)

    // 4. 输出到控制台
    wordCounts.print()

    // 5. 启动任务
    env.execute("Flink Streaming Word Count")
  }
}



执行步骤
nc -lk 9999
flink run -c WordCountStream wordcount.jar
```

**2.2使用 Flink DataSet API（批处理）**

```scala
import org.apache.flink.api.scala._

object WordCountBatch {
  def main(args: Array[String]): Unit = {
    // 1. 创建 Flink 执行环境
    val env = ExecutionEnvironment.getExecutionEnvironment

    // 2. 读取文本文件（HDFS 或本地文件）
    val text: DataSet[String] = env.readTextFile("hdfs://path/to/input")

    // 3. 处理数据
    val wordCounts = text
      .flatMap(_.toLowerCase.split("\\s+"))
      .map((_, 1))
      .groupBy(0)
      .sum(1)

    // 4. 输出到文件或控制台
    wordCounts.writeAsText("hdfs://path/to/output")

    // 5. 执行任务
    env.execute("Flink Batch Word Count")
  }
}
```

**2.3Flink Word Count（Table API 方式）**

```scala
import org.apache.flink.table.api._
import org.apache.flink.table.api.bridge.scala._
import org.apache.flink.streaming.api.scala._

object WordCountTable {
  def main(args: Array[String]): Unit = {
    // 1. 创建流式环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val tableEnv = StreamTableEnvironment.create(env)

    // 2. 读取 Socket 数据流
    val textStream = env.socketTextStream("localhost", 9999)

    // 3. 转换为表
    val table = tableEnv.fromDataStream(
      textStream.flatMap(_.split("\\s+")).map((_, 1))
    ).as("word", "count")

    // 4. 执行 SQL 查询
    val resultTable = table
      .groupBy($"word")
      .select($"word", $"count".sum())

    // 5. 转换为数据流并打印
    tableEnv.toChangelogStream(resultTable).print()

    // 6. 执行
    env.execute("Flink Table API Word Count")
  }
}
```



# 英语面试

## 1.自我介绍

```markdown
Thank you for giving me the opportunity to introduce myself.

Good afternoon! I’m Olivia, and I’m excited to be here for this interview.

I hold a bachelor’s degree in Mathematics and Applied Mathematics from Hefei University. I have over six years of experience as a data engineer.

One of the most significant experiences in my career was working as a Data Engineer on eBay’s Shanghai Data Warehouse team for the past three years. During this time, I had the opportunity with a top-tier technical team, which enabled me to gain extensive hands-on experience and deepen my understanding of data engineering best practices.

My mainly focus was on data integration for both commercial and product domains. Additionally, I contributed to several key projects, including data governance initiatives and performance tuning for production jobs. These experiences enhanced my problem-solving skills and my ability to optimize large-scale data processing workflows.

Due to a strategic restructuring, eBay decided to close its Shanghai office, which is why I am currently seeking new opportunities.


I am proficient in the necessary skills as a data development engineer.including Hadoop, Spark, and Hive, and I have over three years of experience working with AWS cloud-based solutions. moreover, I also have rich experience in data processes performance tuning.

I think my skills and experience are align well with this position.And I belive my experience can bring value to your team.I’m very much looking forward to working with you,Thank you!
```

## 2.Project introduction

### CDP

```
Certainly, I’d like to discuss a challenging project I worked on called the “Consolidated Data Product” (CDP). This project was initiated during a critical phase of our company's business transformation. At that time, multiple European branches had inconsistent data definitions, making it difficult for the headquarters to obtain a comprehensive view of operations. To address this, we proposed building a unified data product to standardize KPIs, optimize data analysis dimensions, and support efficient decision-making.

The architecture of this project was built on AWS, BigQuery, Spark, and Tableau. Our data sources included existing DWD-level tables from our data warehouse and business databases hosted on BigQuery. Spark served as our processing engine, and the final metrics were visualized through Tableau dashboards.

In this project, I was primarily responsible for calculating all key metrics under the Transaction and Advertising domains. One of the main challenges was to rapidly align with the business team on the critical KPIs. During the requirements analysis phase, I ensured a thorough understanding of the business logic behind these metrics, translating them accurately into technical specifications. I then designed a layered data architecture that divided the data into ODS, DWD, DWS, and AWS layers, and developed the corresponding data processing workflows.

To address performance issues, I optimized several tasks within the pipeline. For example, by adjusting resource allocation and optimizing operations such as the count distinct function, we were able to reduce the runtime of some tasks dramatically—from 6 hours down to 1 hour. In addition, I integrated a comprehensive data quality management process into the workflow. This involved setting up validation tasks to ensure data timeliness, accuracy, completeness, consistency, and uniqueness throughout the pipeline.

Finally, I deployed these tasks to our production environment using Airflow for daily scheduling, ensuring a robust and reliable data pipeline.

This project was both challenging and rewarding, as it not only enhanced my technical skills across AWS, BigQuery, Spark, and Tableau but also sharpened my ability to collaborate with business stakeholders to deliver critical, data-driven insights.

I’m very much looking forward to working with you, leveraging my skills and extensive experience to contribute my share to the team. Thank you!
```

### Data governance

```
Project Background:
The goal of the project was to ensure efficient, secure, compliant, and valuable management of organizational data to support data-driven decision-making and business operations. By improving data quality, ensuring data security, maintaining compliance, and enhancing data availability, data governance helps maximize the value of data while minimizing the associated risks and costs.

Responsibilities and Key Tasks:

Metadata Management:

I built a metadata management framework to address the large number of unstructured and unlayered tables in the data warehouse. My tasks included:

Designing and implementing a data processing workflow to collect and maintain metadata for all tables in the platform.
Creating a dictionary table containing detailed information such as table names, fields, partitions, table types, business attributes, storage paths, table sizes, data lineage, associated processes, development owner (Dev Owner), and project manager (PM Owner).
Developing an integration data flow for a Data Asset (DA) management tool that requires any new table to be reviewed and approved via GitHub before it can be created. This tool ensures the standardization and regulation of the table creation process, with scripts that automatically collect relevant metadata.
This framework significantly improved table management efficiency, optimized data accessibility, and provided insights into business logic.
Query Log Cost Analysis and Optimization:

To optimize costs, I used the Databricks API to collect daily query execution information from the platform. This included:

Estimating the cost of each query and creating a foundational Query Log table to store the data.
Regularly optimizing high-cost queries through SQL tuning, which led to reduced platform operating costs.
Query Behavior Analysis and Process Optimization:

Based on the Query Log table, I analyzed the queries to extract details such as the tables and users involved in each query. The optimization process included:

Analyzing table usage frequency and mapping it to actual business use cases.
Performing in-depth optimizations at both the table and data process levels, resulting in improved overall system performance and efficiency.
```

# 数据治理

包含哪些内容：
1.数据质量管理:
标准：
及时性：最新的数据及时到达，没有延迟
完整性：数据无丢失，如数据条数无异常，字段无丢失
唯一性：无重复数据
准确性：结果值准确
一致性：数据在不同存储中一致，无歧义

验证流程：
data pipline(在数据流中添加数据质量验证task验证数据准确值，如有异常则block数据流程，抛出异常，防止异常数据写入目标表，assert_true)
airflow monitor(数据流监控，保证数据流正常运行，当数据流存在失败task及longrun时会展示在监控平台)
data_reliability(定时调度，批量验证数据表)
tableau 报表（根据业务规则验证数据准确性）


2.数据安全及隐私管理
权限管理；s3权限，表权限
数据加密：
pii字段（user_id,email,phone_number等信息）：without_pii_view+pii_view(权限管理)
GDPR:加到数据流程中删除GDPR人员数据（关联gdpr表，将敏感数据移除）



3.数据生命周期管理
3.1数据创建与收集
数据采集标准
数据源管理
3.2数据存储管理
存储管理：确定数据存储标准及格式（比如数据存储在哪里，数据文件什么格式）
数据备份：定期对重要的表进行备份,生成快照表
3.3数据归档与数据销毁
数据归档：对冷数据归档至低成本存储
数据销毁：删除不需要的数据（判断依据：databricks query log表中无use case,pm，dev,user三方sign off后删除数据）

4.元数据管理
元数据是描述数据的数据，包含数据结构，数据来源，用途，质量等信息，元数据管理帮助组织了解其数据资产，保证数据能够被正确和有效地使用。
元数据定义：
业务元数据：描述数据的业务意义，如字段含义，数据来源，数据用途，数据归属等；
技术元数据：描述数据的技术特征，如数据类型，数据存储，数据更新策略，数据模型等；
数据血缘分析（sql解析）

5.数据架构与数据模型管理
5.1数据架构设计
5.2数据建模
5.3数据标准化
统一数据格式，单位，命名规范



## 元数据管理

**元数据管理（Metadata Management）** 是数据治理（Data Governance）的关键组成部分，它负责管理、组织和利用与数据相关的元数据，以提升数据的可用性、可理解性和一致性。一个完善的 **元数据管理** 体系能够帮助企业提升数据质量、加强数据安全，并提高数据分析的效率。

------

**1. 什么是元数据？**

**元数据（Metadata）** 就是“关于数据的数据”，主要用于描述、解释和管理数据，通常分为以下三类：

| **元数据类型**                         | **描述**                                                   | **示例**                                           |
| -------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------- |
| **技术元数据（Technical Metadata）**   | 描述数据的结构、存储方式、数据类型、数据源等               | 表结构、数据类型、索引、分区、存储路径、ETL 任务等 |
| **业务元数据（Business Metadata）**    | 业务视角的元数据，描述数据的业务含义、业务规则、指标定义等 | 客户 ID（客户唯一标识）、订单状态（已支付/未支付） |
| **操作元数据（Operational Metadata）** | 记录数据的访问、变更、质量状态等信息                       | 数据更新时间、数据流转路径、数据权限、数据质量评分 |

------

**2. 元数据管理的核心功能**

在数据治理体系中，**元数据管理** 主要包括以下核心功能：

1. **元数据采集（Metadata Ingestion）**：
   - 从 **数据库**、**数据仓库**、**ETL 作业**、**API** 等数据源自动提取元数据。
   - 例如，从 MySQL 数据库获取表结构信息，从 Kafka 获取数据流动信息。
2. **元数据存储（Metadata Storage）**：
   - 统一存储元数据信息，通常采用 **关系型数据库（MySQL、PostgreSQL）** 或 **NoSQL（ElasticSearch、Neo4j）** 作为元数据存储库。
3. **元数据管理（Metadata Management）**：
   - 提供数据血缘（Data Lineage）分析，展示数据如何从源头流转到下游应用。
   - 进行数据分类和标签化，如 **敏感数据分类（PII、金融数据）**。
4. **元数据查询（Metadata Search & Discovery）**：
   - 允许用户搜索数据资产，例如“查找所有包含‘销售’字段的表”。
   - 通过 **数据目录（Data Catalog）** 提供数据可视化。
5. **数据血缘（Data Lineage）**：
   - 跟踪数据从数据源到最终使用（如报表）的路径，帮助理解数据变更的影响。
   - 例如：`销售订单数据` 从 `MySQL` 进入 `Hadoop`，经过 `ETL` 处理后进入 `数据仓库`，最终被 `BI 工具` 使用。
6. **数据影响分析（Impact Analysis）**：
   - 当数据库字段发生变更时，自动识别可能受到影响的表、ETL 作业或报表。
   - 例如，如果 `customer_email` 字段被删除，需要分析哪些下游报表会出错。
7. **数据质量监控（Data Quality Monitoring）**：
   - 结合元数据进行 **数据质量校验**，如字段缺失率、数据一致性检查等。
8. **数据权限与安全管理（Data Governance & Security）**：
   - 通过元数据定义 **访问权限**，确保敏感数据只能由授权用户访问。
   - 例如，财务数据只能被财务部门访问，个人隐私数据（PII）需要加密存储。

------

**3. 案例：电商企业的元数据管理实施**

**背景**

某电商公司希望优化其 **数据治理体系**，以更好地管理数据资产，提升数据质量，并减少由于数据变更导致的错误。他们的数据存储和分析环境包括：

- **业务数据库（MySQL）**：存储用户、订单、商品等核心业务数据。
- **大数据平台（Hadoop + Hive）**：用于存储和处理大规模日志数据。
- **数据仓库（Amazon Redshift / Snowflake）**：用于分析与 BI 报表。
- **BI 工具（Tableau / Power BI）**：用于数据可视化和业务分析。

------

**解决方案**

1. **建立元数据管理平台**

   - 采用 **Apache Atlas** 或 **DataHub** 作为 **元数据管理工具**，或者自建元数据存储库。
   - 搭建 **MySQL / PostgreSQL** 作为 **元数据存储库**。

2. **元数据采集**

   - 通过 

     ETL 工具（Apache Nifi、Airflow、Flink）

      自动从 MySQL、Hive、Kafka 采集元数据：

     - 提取 **表结构（表名、字段、数据类型）**。
     - 记录 **数据流转路径（Data Lineage）**。
     - 监控 **数据变更（表新增、字段删除）**。

3. **建立数据目录（Data Catalog）**

   - 在 

     DataHub / Apache Atlas

      中创建 

     数据资产目录

     ，包含：

     - **表级元数据**（数据存放位置、创建时间、更新时间）。
     - **字段级元数据**（字段名称、数据类型、字段描述）。
     - **业务指标**（GMV、活跃用户数、退货率等）。

4. **数据血缘分析**

   - 通过 **Data Lineage** 追踪 `订单数据` 从 **MySQL → Hive → 数据仓库 → BI 工具** 的全流程。
   - 当 **MySQL 表字段变更** 时，能立即分析哪些 **ETL 作业、数据仓库表、BI 报表** 可能会受影响。

5. **数据质量监控**

   - 结合 

     Great Expectations

      或 

     Apache Griffin

      进行 

     数据质量检查

     ：

     - **完整性检查**：监控字段 `email` 是否有 NULL 值。
     - **一致性检查**：监控 `订单金额` 是否与 `支付金额` 一致。
     - **数据延迟监控**：检测 Hive 数据是否在 10 分钟内同步到数据仓库。

6. **数据安全与合规**

   - 设定 

     数据权限控制

     ：

     - **普通员工** 只能查看聚合数据，不能访问详细订单数据。
     - **财务部门** 需要访问 **用户支付数据**，但不能查看 **用户邮箱**。

   - 对 **个人数据（PII）** 进行 **脱敏**（如 `手机号` 仅显示 `****1234`）。

   - 监控 **敏感数据访问**，记录访问日志。

------

**4. 实施效果**

✅ **提升数据可见性**：用户可以通过 **Data Catalog** 轻松查找数据资产，不再需要手动询问数据来源。
✅ **降低数据变更风险**：通过 **数据血缘分析**，在变更数据库结构前，系统能自动提醒 **哪些报表和作业可能会出错**。
✅ **提高数据质量**：自动检测数据异常，减少 BI 报表错误率。
✅ **加强安全性**：控制敏感数据的访问权限，符合 **GDPR、CCPA** 等数据法规要求。

------

**5. 总结**

元数据管理是数据治理的重要组成部分，能够帮助企业 **更高效地管理数据资产，提高数据质量，降低数据变更风险，并确保数据安全**。

- 通过 **数据目录（Data Catalog）**，可以提升数据的可发现性。
- 通过 **数据血缘（Data Lineage）**，可以跟踪数据的全生命周期。
- 通过 **数据质量监控**，可以确保数据的完整性、一致性和准确性。



# 如何进行需求分析

## **1.理解业务背景**

目标：明确业务的现状和目标

**了解行业和公司背景**：熟悉公司核心业务及业务模式；

明确目标：明确业务的核心需求是什么，了解经营情况，增加用户留存率，提高内容推荐精准度，优化运营效率？确认是长期目标还是短期目标。

## **2.沟通和需求收集**

目标：从业务方或利益相关者那里获取信息。

**提出清晰的问题：**

```
需要解决什么问题？（如会员流失率高，直播用户参与度低等）
预期低结果是什么？（如生成阅读报表，构建实时监控系统等）
现有的痛点是什么（如数据延迟高，报表不准确等）
```

**访谈和会议：**组织跨部门会议，确保技术团队和义务团队对需求的理解一致。

**记录关键点：**用文档记录需求背景，目标和期望。

## 3.数据分析视角解构需求

目标：将业务需求转化为数据需求。

**1.从业务问题拆解为数据问题**

```
业务问题：如何提高音频推荐的精确度？哪些指标可以反应子公司的经营情况
数据问题：哪些指标，哪些维度对数据可以给出建议
```

**2.定义指标和维度**

```
核心指标
指标维度
```

**3.找到数据来源**

确认所需数据是否已有，存储在哪些表里，表的数据粒度，表的基本信息有哪些，是否需要新建数据采集流程？

**4.明确技术和系统需求**

目标：明白技术团队需要完成的任务

明确数据处理方式：是批处理还是实时处理

选择技术工具

定义SLA

**5.明确优先级**

目标：根据资源和时间安排，合理规划任务

与业务方沟通：确定哪些需求是核心和紧急的

优先解决高价值场景

指定里程碑：设定交付时间，分阶段完成需求。

**6.建模与验证**

目标：验证需求分析是否正确

数据建模：

```
确定用什么建模方式支持需求：维度建模还是3NF。
设计事实表和维度表。
开发数据处理流程。
```

验证数据：

```
与业务方确认结果是否满足预期；
通过小范围测试，验证数据质量和指标有效性；
```

## 4.交付解决方案

目标：转化需求为可交付内容

可交付成果：

数据报表，API接口。

清晰的文档说明，包括使用方式和限制。

反馈迭代：收集业务方一件，持续优化和调整。



# 职业规划

**短期（1-3年）：**

1.提升技术深度与广度：

深耕大数据技术栈：结合工作深入研究技术框架，学习框架优化，关注技术细节，多思考总结积累经验，

跨领域学习：保持学习的态度，学习流行框架，扩展技术广度，同时可以考虑跨界学习一些与数据开发相关的其他领域知识，如人工智能、机器学习等，拓展自己在数据科学领域的知识。

2.提升软实力：

项目管理和团队合作：增强沟通能力，团队协作能力，参与更多的跨部门协作

代码质量和开发效率：提升代码质量，优化开发效率，总结经验

3.提升业务理解：不做大头兵，深入了解所在行业的业务背景、业务流程、数据需求、关键指标等，提升在特定领域的专业性；

**中期（3-5年）：**

4.参与架构设计：结合实际业务需求设计出高效，可扩展的数据处理架构（掌握分布式计算，数据存储，负载均衡，容错机制等知识）

5.担任技术负责人：主导技术选型，评估不同工具和技术的优缺点，进行技术架构决策；

6.领导技术团队：管理项目开发团队，负责人员培养，项目管理，进度把控等，帮助团队成员成长

**长期（5-10年）：**

7.成为数据架构师或数据科学家；

8.向更高职位发展；
